# Copyright 2021 Prayas Energy Group(https://www.prayaspune.org/peg/)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""utilities used by all common, demand and supply.
this should not depend on common,demand or supply modules.
"""
from io import StringIO
import datetime
import pandas as pd
from rumi.io import loaders
from rumi.io import constant
from rumi.io.logger import logging
from rumi.io import functionstore as fs
import numpy as np
import itertools

logger = logging.getLogger(__name__)


def get_all_structure_columns(data, index_cols=None):
    if not index_cols:
        index_cols = []
    c = get_consumer_columns_from_dataframe(data)
    g = get_geographic_columns_from_dataframe(data)
    t = get_time_columns_from_dataframe(data)
    return index_cols+c+g+t


def base_dataframe_all(conscols=None,
                       geocols=None,
                       timecols=None,
                       demand_sector=None,
                       colname="dummy",
                       val=np.nan):
    """Generates base dataframe with all possible rows for time, geography and consumers.

    :param: conscols : list
            Consumer column names
    :param: geocols : list
            Geography column names
    :param: timecols : list
            Time column names
    :param: demand_sector : str
            Demand sector required if conscols is given
    :param: colname : str
            Additional column name, default is "dummy"
    :param: val : object
            Value in additional column, default is np.nan
    :returns:
        a DataFrame. Will be generated by making use of parameters from Common

    """
    if not conscols:
        conscols = []
        cp = [tuple()]
    else:
        cp = compute_product_consumers(conscols, demand_sector)
    if not geocols:
        geocols = []
        gp = [tuple()]
    if not timecols:
        timecols = []
        tp = [tuple()]

    gp = compute_product_geo(geocols)
    tp = compute_product_time(timecols)

    def tup(x):
        return x if isinstance(x, tuple) else (x,)

    p = [tup(c)+tup(g)+tup(t) for c in cp for g in gp for t in tp]

    names = conscols + geocols + timecols
    if len(names) == 1:
        index = pd.index([item[0] for item in p], names=names)
    else:
        index = pd.MultiIndex.from_tuples(p,
                                          names=names)
    rows = len(p)

    return pd.DataFrame({colname: [val]*rows},
                        index=index)


def unique_across(data, columns):
    """checks if given columns combination has unique items
    """
    geocols = get_geographic_columns_from_dataframe(data)
    timecols = get_time_columns_from_dataframe(data)
    conscols = get_consumer_columns_from_dataframe(data)
    cols = geocols + timecols + conscols + columns
    return len(data) == len(data.drop_duplicates(subset=cols))


def find_interval(start, end):
    m1, d1 = start
    m2, d2 = end
    y = 2019  # non leap year
    interval = datetime.datetime(y, m2, d2) - datetime.datetime(y, m1, d1)
    if interval.days < 0:
        return interval.days + 365
    return interval.days


def get_valid_time_levels():
    """Checks files defined in Common and decides what time levels to return
    """
    if isinstance(loaders.get_parameter("DaySlices"), pd.DataFrame):
        levels = [s.upper() for s in constant.TIME_SLICES]
    elif isinstance(loaders.get_parameter("DayTypes"), pd.DataFrame):
        levels = [s.upper() for s in constant.TIME_SLICES[:3]]
    elif isinstance(loaders.get_parameter("Seasons"), pd.DataFrame):
        levels = [s.upper() for s in constant.TIME_SLICES[:2]]
    else:
        levels = [s.upper() for s in constant.TIME_SLICES[:1]]

    return levels


def get_valid_geographic_levels():
    """returns valid geographic levels in system.
    checks actual files defined in common and accordingly
    find valid geographic levels.

    Returns
    -------
    a list containing valid geographic granularities
    """
    if loaders.get_parameter("SubGeography3"):
        levels = [s.upper() for s in constant.GEOGRAPHIES]
    elif loaders.get_parameter("SubGeography2"):
        levels = [s.upper() for s in constant.GEOGRAPHIES[:3]]
    elif loaders.get_parameter("SubGeography1"):
        levels = [s.upper() for s in constant.GEOGRAPHIES[:2]]
    else:
        levels = [s.upper() for s in constant.GEOGRAPHIES[:1]]

    return levels


def get_pair(season):
    """return month and date pair
    """
    return season['StartMonth'], season['StartDate']


def get_geographic_columns(geographic_granularity):
    return constant.GEO_COLUMNS[geographic_granularity]


def get_time_columns(time_granularity):
    return constant.TIME_COLUMNS[time_granularity]


def compute_intervals(Seasons):
    seasons = Seasons.to_dict(orient='records')
    seasons.append(seasons[0])
    return {s['Season']: find_interval(get_pair(s), get_pair(seasons[i+1]))
            for i, s in enumerate(seasons[:-1])}


def seasons_size():
    Seasons = loaders.get_parameter("Seasons")
    return compute_intervals(Seasons)


def valid_geography(dataframe):
    """This checks validity of geography columns, this works
    only for datasets in which there is no other repeating entity.
    so this will work only for for data like GDP and DemoGraphics
    """
    geocols = get_geographic_columns_from_dataframe(dataframe)
    timecols = get_time_columns_from_dataframe(dataframe)
    basegeodata = base_dataframe_geography(geocols).reset_index()
    base = basegeodata[geocols].sort_values(geocols).reset_index(drop=True)
    if not timecols:
        return dataframe[geocols].sort_values(geocols).reset_index(drop=True).eq(base).all().all()
    data = dataframe.set_index(timecols)[geocols]
    valid = True

    for t in data.index.unique():
        d = data.loc[t]
        d = d.sort_values(geocols).reset_index(drop=True)
        valid = valid and d.eq(base).all().all()
    return valid


def filter_by(data, param_name, colname):
    param = loaders.get_parameter(param_name)

    def filter_(x):
        return x in param[colname].values

    return data[data[colname].apply(filter_)]


def filter_empty(data, all=True):
    """filters out empty columns from dataframe"""
    rows = len(data)
    empty = [c for c in data.columns if data[c].isnull(
    ).sum() == rows or (data[c] == "").sum() == rows]
    return data[[c for c in data.columns if c not in empty]]


def get_geographic_columns_from_dataframe(data):
    return [c for c in data.columns if c in constant.GEOGRAPHIES]


def get_consumer_columns_from_dataframe(data):
    return [c for c in data.columns if c in constant.CONSUMER_TYPES]


def get_time_columns_from_dataframe(data):
    return [c for c in data.columns if c in constant.TIME_SLICES]


def group_daytype(data, geogroup, target):
    """aggregates data by DayType considering weights of
    """
    if isinstance(target, list):
        targets = target
    else:
        targets = [target]

    if 'DayType' in data.columns:
        DayTypes = loaders.get_parameter('DayTypes')
        weights = DayTypes.set_index("DayType")['Weight']

        data = data.set_index(geogroup + ['Year', 'Season', 'DayType'])
        for item in targets:
            data[item] = data[item]*weights

        return data.groupby(geogroup +
                            ['Year',
                             'Season',
                             'DayType'], sort=False).sum(numeric_only=True)[targets].reset_index()
    else:
        return data


def group_season(data, geogroup, target):
    """aggregates target column by considering number of days in season.
    """
    if isinstance(target, list):
        targets = target
    else:
        targets = [target]

    if 'Season' in data.columns:
        seasons_size_ = pd.Series(seasons_size())
        seasons_size_.index.rename('Season', inplace=True)

        data = data.set_index(geogroup + ['Year', 'Season'])
        for item in targets:
            data[item] = data[item]*seasons_size_
        data = data.reset_index()
        return data.groupby(geogroup +
                            ['Year', 'Season'], sort=False).sum(numeric_only=True)[targets].reset_index()
    else:
        return data


def group_year(data, geogroup, target):
    """ aggregates target column by year
    """
    if isinstance(target, list):
        targets = target
    else:
        targets = [target]

    if 'Season' in data.columns:
        timecols = ['Year']
        data = data.groupby(geogroup + timecols,
                            sort=False).sum(numeric_only=True)[targets].reset_index()
    return data


def groupby_time(data,
                 groupcols,
                 balancing_time,
                 target=None):

    if not target:
        target = [c for c in data if data[c].dtype == np.float64]

    if balancing_time == 'YEAR':
        data = group_daytype(data, groupcols, target)
        data = group_season(data, groupcols, target)
        data = group_year(data,
                          groupcols,
                          target)
        return data

    elif balancing_time == 'SEASON':
        data = group_daytype(data, groupcols, target)
        return group_season(data, groupcols, target)
    elif balancing_time == 'DAYTYPE':
        return group_daytype(data, groupcols, target)
    else:
        return data


def make_list(target):
    if isinstance(target, list):
        return target
    else:
        return [target]


def groupby(data,
            groupcols,
            target):
    """time columns needs to be handled specially, so this is
    alternative to DataFrame.groupby. One should always have
    at least coarsest time column in groupcols
    """
    # TODO: This needs unittest. many calculations depend on this
    othercols = [c for c in groupcols if c not in constant.TIME_SLICES]
    targets = make_list(target)
    if 'DaySlice' in groupcols:
        r = data
    elif 'DayType' not in groupcols and\
        'Season' not in groupcols and\
            'Year' in groupcols:
        r1 = group_daytype(data, othercols, targets)
        r2 = group_season(r1, othercols, targets)
        r = group_year(r2,
                       othercols,
                       targets)

    elif "DayType" not in groupcols and \
         'Season' in groupcols:
        r = group_daytype(data, othercols, targets)
        r = group_season(r, othercols, targets)
    elif 'DayType' in groupcols:  # DaySlice not in groupcols implicit meaning
        # check if condition
        r = group_daytype(data, othercols, targets)
    else:
        raise Exception("Invalid groupby columns")

    if r is data:
        r = data.groupby(groupcols, sort=False)[targets].sum().reset_index()

    r = r.set_index(get_all_structure_columns(r))

    return r[target]


def order_by(v):
    flag = True
    if v.name == 'Season':
        order = get_order('Season', 'Seasons')
    elif v.name == 'DayType':
        order = get_order('DayType', 'DayTypes')
    elif v.name == 'DaySlice':
        order = get_order('DaySlice', 'DaySlices')
    elif v.name == 'SubGeography1':
        order = geo_order('SubGeography1')
    elif v.name == 'SubGeography2':
        order = geo_order('SubGeography2')
    elif v.name == 'SubGeography3':
        order = geo_order('SubGeography3')
    else:
        flag = False

    if flag:
        return v.apply(order.index)

    return v


def get_order(col, param):
    parameter = loaders.get_parameter(param)
    return list(parameter[col].values) + [np.nan]


def geo_order(col):
    parameter = loaders.get_parameter(col)
    if col == 'SubGeography1':
        return parameter + [np.nan]
    else:
        return sum(parameter.values(), []) + [np.nan]


def get_ordered_cols(df):
    all_cols = constant.TIME_SLICES + constant.GEOGRAPHIES + constant.CONSUMER_TYPES
    TGC_cols = [c for c in all_cols if c in df.columns]
    remaining_cols = [c for c in df.columns if c not in TGC_cols]
    return TGC_cols + remaining_cols


def order_rows(df):
    """orders dataframe rows by sorting by T*,G*,C* and order to be taken as
    specified in common specs.
    e.g. Order of Seasons will be taken from "Seasons' parameter
    Order of DayType will be taken from "DatTypes' parameter
    Order of  DaySlice will be taken from "DaySlices' parameter
    SubGeography1 order will be taken as given in 'SubGeography1' parameter
    SubGeography2 order will be taken as given in 'SubGeography2' parameter
    SubGeography3 order will be taken as given in 'SubGeography3' parameter
    """
    all_cols = constant.TIME_SLICES + constant.GEOGRAPHIES
    dataset_cols = [c for c in all_cols if c in df.columns]
    return df.sort_values(by=dataset_cols, key=order_by, ignore_index=True)


def get_modelgeography_product():
    data = {c: loaders.get_parameter(c) for c in ['ModelGeography']}
    return (data['ModelGeography'],)


def get_subgeography1_product():
    data = {c: loaders.get_parameter(c)
            for c in ['ModelGeography', 'SubGeography1']}
    return [(data['ModelGeography'], item) for item in data['SubGeography1']]


def get_subgeography2_product():
    data = {c: loaders.get_parameter(c)
            for c in ['ModelGeography', 'SubGeography1', 'SubGeography2']}
    x = [(data['ModelGeography'], item) for item in data['SubGeography1']]
    l = []
    for item1 in x:
        l.extend([item1 + (item2,)
                  for item2 in data['SubGeography2'][item1[-1]]])
    return l


def get_subgeography3_product():
    data = {c: loaders.get_parameter(c)
            for c in ['ModelGeography', 'SubGeography1', 'SubGeography2', 'SubGeography3']}
    x = [(data['ModelGeography'], item) for item in data['SubGeography1']]
    l = []
    for item1 in x:
        l.extend([item1 + (item2,)
                  for item2 in data['SubGeography2'][item1[-1]]])

    m = []
    for item1 in l:
        m.extend([item1 + (item2,)
                  for item2 in data['SubGeography3'][item1[-1]]])
    return m


def compute_product_geo(geocols):

    if len(geocols) == 1:
        return get_modelgeography_product()
    elif len(geocols) == 2:
        return get_subgeography1_product()
    elif len(geocols) == 3:
        return get_subgeography2_product()
    elif len(geocols) == 4:
        return get_subgeography3_product()


def base_dataframe_geography(geocols, colname="dummy", val=np.nan):
    """Useful for creating dataframes of given gegraphic columns
    """
    p = compute_product_geo(geocols)

    rows = len(p)
    if len(geocols) == 1:
        index = pd.Index(p, name=geocols[0])
    else:
        index = pd.MultiIndex.from_tuples(p,
                                          names=geocols)
    return pd.DataFrame({colname: [val]*rows},
                        index=index)


def get_years():
    modelperiod = loaders.get_parameter('ModelPeriod').iloc[0]
    years = range(modelperiod['StartYear'], modelperiod['EndYear']+1)
    return years


def get_seasons():
    return tuple(loaders.get_parameter('Seasons')['Season'].values)


def get_daytypes():
    return tuple(loaders.get_parameter('DayTypes')['DayType'].values)


def get_dayslices():
    return tuple(loaders.get_parameter('DaySlices')['DaySlice'].values)


def compute_product_time(timecols):
    if len(timecols) == 1:
        return get_years()
    elif len(timecols) == 2:
        return list(itertools.product(get_years(),
                                      get_seasons()))
    elif len(timecols) == 3:
        return list(itertools.product(get_years(),
                                      get_seasons(),
                                      get_daytypes()))
    elif len(timecols) == 4:
        return list(itertools.product(get_years(),
                                      get_seasons(),
                                      get_daytypes(),
                                      get_dayslices()))


def base_dataframe_time(timecols, colname='dummy', val=np.nan):
    """Useful for creating datasets of given time columns
    """
    p = compute_product_time(timecols)

    rows = len(p)
    if len(timecols) == 1:
        index = pd.Index(p, name=timecols[0])
        # merge with multiindex dataset fails!
    else:
        index = pd.MultiIndex.from_tuples(p,
                                          names=timecols)
    return pd.DataFrame({colname: [val]*rows},
                        index=index)


def make_dataframe(datastr):
    with StringIO(datastr) as f:
        return pd.read_csv(f)


def check_granularity_per_entity(d,
                                 entity,
                                 GeographicGranularity,
                                 TimeGranularity,
                                 ConsumerGranularity=None):
    """check if the entity specified is exactly equal to granularities specified
    """
    geo_columns, time_columns, cons_columns = [], [], []

    if GeographicGranularity:
        geo_columns = get_geographic_columns(GeographicGranularity)
        dataset_columns = [c for c in d.columns if c in constant.GEOGRAPHIES]
    if TimeGranularity:
        time_columns = get_time_columns(TimeGranularity)
        dataset_columns.extend(
            [c for c in d.columns if c in constant.TIME_SLICES])
    if ConsumerGranularity:
        cons_columns = constant.CONSUMER_COLUMNS[ConsumerGranularity]
        dataset_columns.extend(
            [c for c in d.columns if c in constant.CONSUMER_TYPES])

    diff1 = set(geo_columns + time_columns +
                cons_columns) - set(dataset_columns)
    diff2 = set(dataset_columns) - \
        set(geo_columns + time_columns + cons_columns)

    valid = True
    if diff1:
        logger.error(f"{diff1} not found in data for {entity}")
        valid = False
    else:
        # redundant!
        nonempty = d[geo_columns+time_columns].isnull().sum().sum()
        valid = nonempty == 0

    if diff2:
        c, r = d[list(diff2)].shape
        empty = d[list(diff2)].isnull().sum().sum() == c*r

        if not empty:
            logger.error(f"Granularity is finer than expected for {entity}!")
        valid = valid and empty

    return valid


def get_consumertype2_product(demand_sector):
    consumertype1 = get_consumertype1_product(demand_sector)
    Cons1_Cons2_Map = loaders.get_parameter("Cons1_Cons2_Map")

    return sum([[(t1, t2) for t2 in Cons1_Cons2_Map.get(t1, [])] for t1 in consumertype1], [])


def get_consumertype1_product(demand_sector):
    DS_Cons1_Map = loaders.get_parameter("DS_Cons1_Map")
    return DS_Cons1_Map[demand_sector][2:]


def compute_product_consumers(conscols, demand_sector):
    if len(conscols) == 1:
        return get_consumertype1_product(demand_sector)
    elif len(conscols) == 2:
        return get_consumertype2_product(demand_sector)


def base_dataframe_consumers(conscols, demand_sector, colname='dummy', val=np.nan):
    """Useful for creating datasets of given consumer columns
    """
    p = compute_product_consumers(conscols, demand_sector)
    rows = len(p)
    if len(conscols) == 1:
        index = pd.Index(p, name=conscols[0])
        # merge with multiindex dataset fails!
    else:
        index = pd.MultiIndex.from_tuples(p,
                                          names=conscols)
    return pd.DataFrame({colname: [val]*rows},
                        index=index)


def balancing_X(EC, X):
    PhysicalPrimaryCarriers = loaders.get_parameter('PhysicalPrimaryCarriers')
    PhysicalDerivedCarriers = loaders.get_parameter('PhysicalDerivedCarriers')
    NonPhysicalDerivedCarriers = loaders.get_parameter(
        'NonPhysicalDerivedCarriers')

    df = None

    if EC in list(PhysicalPrimaryCarriers.EnergyCarrier):
        df = PhysicalPrimaryCarriers
    elif EC in list(PhysicalDerivedCarriers.EnergyCarrier):
        df = PhysicalDerivedCarriers
    elif EC in list(NonPhysicalDerivedCarriers.EnergyCarrier):
        df = NonPhysicalDerivedCarriers
    else:
        raise Exception(f"No such energy carrier, {EC}!")

    # return df.query(f"EnergyCarrier == '{EC}'")[X].iloc[0]
    xdata = df.set_index('EnergyCarrier')
    return xdata[X][EC]


def balancing_time(EC):
    return balancing_X(EC, "BalancingTime")


def balancing_area(EC):
    return balancing_X(EC, "BalancingArea")


def get_cols_from_dataframe(data, type_):
    f = {"C": [c for c in constant.CONSUMER_TYPES if c in data.columns],
         "G": [c for c in constant.GEOGRAPHIES if c in data.columns],
         "T": [c for c in constant.TIME_SLICES if c in data.columns]}
    return f[type_]


def get_base_dataframe(cols, type_, demand_sector=None):
    f = {"C": base_dataframe_consumers,
         "G": base_dataframe_geography,
         "T": base_dataframe_time}
    if type_ == 'C':
        return f[type_](cols, demand_sector, val=0)
    return f[type_](cols, val=0)


def check_CGT_validity(data,
                       name,
                       entity,
                       type_,
                       demand_sector=None,
                       checkunique=True,
                       exact=False):
    """
    check if consumertype, geographies and time columns have appropriate values
    """

    if isinstance(entity, str):
        entity = [entity]

    valid = True
    timecols = get_time_columns_from_dataframe(data)
    conscols = get_consumer_columns_from_dataframe(data)
    geographiccols = get_geographic_columns_from_dataframe(data)
    allcols = {"C": conscols, "G": geographiccols, "T": timecols}
    indexcols = entity + \
        fs.flatten([v for k, v in allcols.items() if k != type_])

    df = data.set_index(indexcols).sort_index()

    for item in df.index.unique():
        typecols = allcols[type_]
        subset = df.loc[item]
        if isinstance(subset, pd.Series):
            basedf = get_base_dataframe(
                tuple(typecols), type_, demand_sector).reset_index()
            v = basedf[typecols[0]].iloc[0] == subset[typecols[0]]
        else:
            subset = filter_empty(subset[typecols])
            typecols_ = get_cols_from_dataframe(subset, type_)
            basedf = get_base_dataframe(
                tuple(typecols_), type_, demand_sector).reset_index()
            if exact:
                del basedf['dummy']
                cols = list(basedf.columns)
                subset = subset.sort_values(by=cols).reset_index(drop=True)
                v = subset.equals(
                    basedf.sort_values(by=cols).reset_index(drop=True))
            else:
                v = subset.isin(basedf.to_dict(
                    orient='list')).all().all()
            if checkunique and len(subset.drop_duplicates()) != len(subset):
                logger.error(
                    f"{name} parameter for {item} has duplicate rows in {typecols} columns")
                v = False
        valid = valid and v
        if not v:
            logger.error(
                f"{name} parameter for {item} has invalid data for {typecols} columns")
    return valid


def check_consumer_validity(data,
                            name,
                            entity,
                            demand_sector=None,
                            checkunique=True,
                            exact=False):
    """
    check if consumertype columns have appropriate values
    """

    return check_CGT_validity(data, name, entity, 'C',
                              demand_sector=demand_sector,
                              checkunique=checkunique,
                              exact=exact)


def check_geographic_validity(data,
                              name,
                              entity,
                              checkunique=True,
                              exact=False):
    """
    check if geographies columns have appropriate values
    """
    return check_CGT_validity(data, name, entity, 'G',
                              checkunique=checkunique,
                              exact=exact)


def check_time_validity(data,
                        name,
                        entity,
                        checkunique=True,
                        exact=False):
    """
    check if time columns have appropriate values
    """
    return check_CGT_validity(data, name, entity, 'T',
                              checkunique=checkunique,
                              exact=exact)


def check_balancing_time_gran(param_name,
                              granmap,
                              entity,
                              comp='coarser',
                              find_EC_=lambda x, y: y[-1]):
    """check if given granularity map specifies granularity appropriately as specified
    by comp parameter.

        Parameters
        ----------
        param_name: str
            parameter name , some granularity map
        granmap: pd.DataFrame
            data for given param_name
        entity: str
            one of EnergyCarrier, EnergyConvTech or EnergyStorTech
        comp: str, default 'coarser'
            one of coarser or finer


        Returns
        -------
        bool
           True if comp is 'finer' and granmap has granularity finer than balancing time
           True if comp is 'coarser' and granmap has granularity coarser than balancing time
           else False

    """
    granmap = granmap.set_index(entity)
    for entity_ in granmap.index:
        ec = find_EC_(entity, entity_)
        balacing_gran = balancing_time(ec)
        data_gran = granmap.loc[entity_]['TimeGranularity']

        if comp == "finer":
            if len(constant.TIME_COLUMNS[balacing_gran]) > len(constant.TIME_COLUMNS[data_gran]):
                logger.error(
                    f"For {param_name} time granularity for {entity},{entity_} is incorrect. It should be finer than balancing time of {ec}")
                return False
        else:
            if len(constant.TIME_COLUMNS[balacing_gran]) < len(constant.TIME_COLUMNS[data_gran]):
                logger.error(
                    f"For {param_name} time granularity for {entity},{entity_} is incorrect. It should be coarser than balancing time of {ec}")
                return False
    return True


def check_balancing_area_gran(param_name,
                              granmap,
                              entity,
                              comp='coarser',
                              find_EC_=lambda x, y: y[-1]):
    """check if given granularity map specifies granularity appropriately as specified
    by comp parameter.

        Parameters
        ----------
        param_name: str
            parameter name , some granularity map
        granmap: pd.DataFrame
            data for given param_name
        entity: str
            one of EnergyCarrier, EnergyConvTech or EnergyStorTech
        comp: str, default 'coarser'
            one of coarser or finer


        Returns
        -------
        bool
           True if comp is 'coarser' and granmap has granularity coarser than balancing area
           True if comp is 'finer' and granmap has granularity finer than balancing area
           else False

    """

    granmap = granmap.set_index(entity)
    for entity_ in granmap.index:
        ec = find_EC_(entity, entity_)
        balacing_gran = balancing_area(ec)
        data_gran = granmap.loc[entity_]['GeographicGranularity']
        if comp == "finer":
            if len(constant.GEO_COLUMNS[balacing_gran]) > len(constant.GEO_COLUMNS[data_gran]):
                logger.error(
                    f"For {param_name} geographic granularity for {entity} is incorrect. It should be finer than balancing area of {ec}")
                return False
        else:
            if len(constant.GEO_COLUMNS[balacing_gran]) < len(constant.GEO_COLUMNS[data_gran]):
                logger.error(
                    f"For {param_name} geographic granularity for {entity}, {entity_} is incorrect. It should be coarser than balancing area of {ec}")
                return False

    return True


if __name__ == "__main__":
    pass
